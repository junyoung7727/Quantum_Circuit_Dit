#!/usr/bin/env python3
"""
Advanced Quantum Circuit Transformer with State-of-the-Art AI Techniques
- Multi-Head Cross-Attention between circuit structure and quantum properties
- Graph Neural Networks for quantum coupling topology
- Hierarchical Transformer with circuit-level and gate-level attention
- Self-supervised pre-training with masked circuit modeling
- Multi-modal fusion of sequential, structural, and physical features
- Uncertainty quantification with Monte Carlo Dropout
- Knowledge distillation from quantum physics priors
- Diffusion Transformer for quantum circuit generation
"""

import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
import json
import gzip
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd
import math
import random
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# ì„¤ì • íŒŒë¼ë¯¸í„°
MAX_SEQUENCE_LENGTH = 100
LATENT_DIM = 512
NUM_HEADS = 16
NUM_LAYERS = 12
BATCH_SIZE = 32
NUM_EPOCHS = 200
LEARNING_RATE = 1e-4
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(f"ğŸš€ Advanced Quantum Transformer ì´ˆê¸°í™”")
print(f"ë””ë°”ì´ìŠ¤: {DEVICE}")
print(f"ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {MAX_SEQUENCE_LENGTH}")
print(f"ì ì¬ ì°¨ì›: {LATENT_DIM}")

#################################################
# 1. Advanced Positional Encoding
#################################################

class QuantumAwarePositionalEncoding(nn.Module):
    """ì–‘ì íšŒë¡œ íŠ¹ì„±ì„ ê³ ë ¤í•œ ìœ„ì¹˜ ì¸ì½”ë”©"""
    
    def __init__(self, d_model: int, max_len: int = MAX_SEQUENCE_LENGTH):
        super().__init__()
        self.d_model = d_model
        
        # ê¸°ë³¸ sinusoidal ìœ„ì¹˜ ì¸ì½”ë”©
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe.unsqueeze(0))
        
        # ì–‘ì ê²Œì´íŠ¸ íƒ€ì…ë³„ í•™ìŠµ ê°€ëŠ¥í•œ ì„ë² ë”©
        self.gate_type_embedding = nn.Embedding(10, d_model)  # 9ê°œ ê²Œì´íŠ¸ íƒ€ì… + íŒ¨ë”©
        
        # íë¹— ìœ„ì¹˜ ì„ë² ë”© (ìµœëŒ€ 127 íë¹—)
        self.qubit_position_embedding = nn.Embedding(128, d_model)
        
        # íšŒë¡œ ê¹Šì´ ì„ë² ë”©
        self.depth_embedding = nn.Embedding(20, d_model)
        
        # ìœµí•© ë ˆì´ì–´
        self.fusion = nn.Linear(d_model * 4, d_model)
        self.dropout = nn.Dropout(0.1)
    
    def forward(self, x, gate_types, qubit_positions, depths):
        batch_size, seq_len = x.size(0), x.size(1)
        
        # ê¸°ë³¸ ìœ„ì¹˜ ì¸ì½”ë”©
        pos_enc = self.pe[:, :seq_len, :]
        
        # ê²Œì´íŠ¸ íƒ€ì… ì„ë² ë”©
        gate_emb = self.gate_type_embedding(gate_types)
        
        # íë¹— ìœ„ì¹˜ ì„ë² ë”©
        qubit_emb = self.qubit_position_embedding(qubit_positions.clamp(0, 127))
        
        # ê¹Šì´ ì„ë² ë”©
        depth_emb = self.depth_embedding(depths.clamp(0, 19))
        
        # ëª¨ë“  ì„ë² ë”© ìœµí•©
        combined = torch.cat([
            pos_enc.expand(batch_size, -1, -1),
            gate_emb,
            qubit_emb,
            depth_emb
        ], dim=-1)
        
        fused = self.fusion(combined)
        return x + self.dropout(fused)

#################################################
# 2. Graph Neural Network for Quantum Topology
#################################################

class QuantumGraphConvolution(nn.Module):
    """ì–‘ì ì»¤í”Œë§ í† í´ë¡œì§€ë¥¼ ìœ„í•œ ê·¸ë˜í”„ í•©ì„±ê³±"""
    
    def __init__(self, in_features: int, out_features: int):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        
        # ìê¸° ìì‹ ê³¼ ì´ì›ƒ ë…¸ë“œë¥¼ ìœ„í•œ ë³„ë„ ë³€í™˜
        self.weight_self = nn.Linear(in_features, out_features)
        self.weight_neighbor = nn.Linear(in_features, out_features)
        
        # ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
        self.attention = nn.MultiheadAttention(out_features, num_heads=8, batch_first=True)
        
        self.norm = nn.LayerNorm(out_features)
        self.dropout = nn.Dropout(0.1)
    
    def forward(self, x, adjacency_matrix):
        batch_size, num_nodes, _ = x.shape
        
        # ìê¸° ë³€í™˜
        self_transform = self.weight_self(x)
        
        # ì´ì›ƒ ë…¸ë“œ ì§‘ê³„
        neighbor_sum = torch.bmm(adjacency_matrix, x)
        neighbor_transform = self.weight_neighbor(neighbor_sum)
        
        # ê²°í•©
        combined = self_transform + neighbor_transform
        
        # ì–´í…ì…˜ ì ìš©
        attended, _ = self.attention(combined, combined, combined)
        
        # ì”ì°¨ ì—°ê²° ë° ì •ê·œí™”
        output = self.norm(attended + combined)
        return self.dropout(output)

class QuantumTopologyEncoder(nn.Module):
    """ì–‘ì íšŒë¡œì˜ í† í´ë¡œì§€ êµ¬ì¡°ë¥¼ ì¸ì½”ë”©í•˜ëŠ” GNN"""
    
    def __init__(self, node_features: int, hidden_dim: int, num_layers: int = 3):
        super().__init__()
        self.num_layers = num_layers
        
        # GCN ë ˆì´ì–´ë“¤
        self.gcn_layers = nn.ModuleList([
            QuantumGraphConvolution(
                node_features if i == 0 else hidden_dim,
                hidden_dim
            ) for i in range(num_layers)
        ])
        
        # ê¸€ë¡œë²Œ í’€ë§
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        self.global_max_pool = nn.AdaptiveMaxPool1d(1)
        
        # ìµœì¢… ë³€í™˜
        self.final_transform = nn.Linear(hidden_dim * 2, hidden_dim)
    
    def forward(self, node_features, adjacency_matrix):
        x = node_features
        
        # GCN ë ˆì´ì–´ë“¤ í†µê³¼
        for gcn in self.gcn_layers:
            x = gcn(x, adjacency_matrix)
        
        # ê¸€ë¡œë²Œ íŠ¹ì„± ì¶”ì¶œ
        x_transposed = x.transpose(1, 2)  # (batch, features, nodes)
        avg_pool = self.global_pool(x_transposed).squeeze(-1)
        max_pool = self.global_max_pool(x_transposed).squeeze(-1)
        
        # ê²°í•©
        global_features = torch.cat([avg_pool, max_pool], dim=-1)
        return self.final_transform(global_features)

#################################################
# 3. Hierarchical Multi-Scale Attention
#################################################

class MultiScaleAttention(nn.Module):
    """ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì–´í…ì…˜ (ê²Œì´íŠ¸ ë ˆë²¨ + ë¸”ë¡ ë ˆë²¨ + íšŒë¡œ ë ˆë²¨)"""
    
    def __init__(self, d_model: int, num_heads: int):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        
        # ê²Œì´íŠ¸ ë ˆë²¨ ì–´í…ì…˜ (ì„¸ë°€í•œ ìƒí˜¸ì‘ìš©)
        self.gate_attention = nn.MultiheadAttention(
            d_model, num_heads, dropout=0.1, batch_first=True
        )
        
        # ë¸”ë¡ ë ˆë²¨ ì–´í…ì…˜ (ì¤‘ê°„ ìŠ¤ì¼€ì¼)
        self.block_attention = nn.MultiheadAttention(
            d_model, num_heads // 2, dropout=0.1, batch_first=True
        )
        
        # íšŒë¡œ ë ˆë²¨ ì–´í…ì…˜ (ì „ì—­ì  íŒ¨í„´)
        self.circuit_attention = nn.MultiheadAttention(
            d_model, num_heads // 4, dropout=0.1, batch_first=True
        )
        
        # ìŠ¤ì¼€ì¼ ìœµí•©
        self.scale_fusion = nn.Linear(d_model * 3, d_model)
        self.norm = nn.LayerNorm(d_model)
    
    def forward(self, x, mask=None):
        batch_size, seq_len, d_model = x.shape
        
        # ê²Œì´íŠ¸ ë ˆë²¨ ì–´í…ì…˜
        gate_out, _ = self.gate_attention(x, x, x, key_padding_mask=mask)
        
        # ë¸”ë¡ ë ˆë²¨ ì–´í…ì…˜ (ìœˆë„ìš° í¬ê¸° 4)
        block_x = self._create_block_representation(x, block_size=4)
        block_out, _ = self.block_attention(block_x, block_x, block_x)
        block_out = self._expand_block_representation(block_out, seq_len)
        
        # íšŒë¡œ ë ˆë²¨ ì–´í…ì…˜ (ì „ì—­ í‰ê· )
        circuit_x = x.mean(dim=1, keepdim=True).expand(-1, seq_len, -1)
        circuit_out, _ = self.circuit_attention(circuit_x, circuit_x, circuit_x)
        
        # ëª¨ë“  ìŠ¤ì¼€ì¼ ìœµí•©
        multi_scale = torch.cat([gate_out, block_out, circuit_out], dim=-1)
        fused = self.scale_fusion(multi_scale)
        
        return self.norm(fused + x)
    
    def _create_block_representation(self, x, block_size):
        batch_size, seq_len, d_model = x.shape
        
        # íŒ¨ë”©í•˜ì—¬ ë¸”ë¡ í¬ê¸°ë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì§€ê²Œ ë§Œë“¤ê¸°
        padded_len = ((seq_len + block_size - 1) // block_size) * block_size
        if padded_len > seq_len:
            padding = torch.zeros(batch_size, padded_len - seq_len, d_model, 
                                device=x.device, dtype=x.dtype)
            x_padded = torch.cat([x, padding], dim=1)
        else:
            x_padded = x
        
        # ë¸”ë¡ìœ¼ë¡œ ì¬êµ¬ì„±
        num_blocks = padded_len // block_size
        blocks = x_padded.view(batch_size, num_blocks, block_size, d_model)
        
        # ë¸”ë¡ ë‚´ í‰ê· 
        block_repr = blocks.mean(dim=2)
        return block_repr
    
    def _expand_block_representation(self, block_repr, target_len):
        batch_size, num_blocks, d_model = block_repr.shape
        
        # ê° ë¸”ë¡ì„ ì›ë˜ ê¸¸ì´ë¡œ í™•ì¥
        expanded = block_repr.unsqueeze(2).expand(-1, -1, 4, -1)
        expanded = expanded.contiguous().view(batch_size, -1, d_model)
        
        # ëª©í‘œ ê¸¸ì´ë¡œ ìë¥´ê¸°
        return expanded[:, :target_len, :]

#################################################
# 4. Physics-Informed Neural Network Components
#################################################

class QuantumPhysicsConstraints(nn.Module):
    """ì–‘ì ë¬¼ë¦¬í•™ ì œì•½ ì¡°ê±´ì„ í•™ìŠµì— ë°˜ì˜"""
    
    def __init__(self, d_model: int):
        super().__init__()
        self.d_model = d_model
        
        # ìœ ë‹ˆí„°ë¦¬ ì œì•½ í•™ìŠµ
        self.unitary_constraint = nn.Linear(d_model, d_model)
        
        # ì—ë¥´ë¯¸íŠ¸ ì œì•½ í•™ìŠµ
        self.hermitian_constraint = nn.Linear(d_model, d_model)
        
        # í™•ë¥  ë³´ì¡´ ì œì•½
        self.probability_constraint = nn.Linear(d_model, 1)
        
        # ë¬¼ë¦¬ì  ì¼ê´€ì„± ê²€ì‚¬
        self.physics_validator = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Linear(d_model // 2, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        # ìœ ë‹ˆí„°ë¦¬ ì œì•½ ì ìš©
        unitary_proj = self.unitary_constraint(x)
        
        # ì—ë¥´ë¯¸íŠ¸ ì œì•½ ì ìš©
        hermitian_proj = self.hermitian_constraint(x)
        
        # í™•ë¥  ë³´ì¡´ ì²´í¬
        prob_conservation = torch.sigmoid(self.probability_constraint(x))
        
        # ë¬¼ë¦¬ì  ì¼ê´€ì„± ì ìˆ˜
        physics_score = self.physics_validator(x)
        
        # ì œì•½ ì¡°ê±´ì„ ë§Œì¡±í•˜ë„ë¡ ì¡°ì •
        constrained_x = x + 0.1 * (unitary_proj + hermitian_proj)
        
        return constrained_x, physics_score, prob_conservation

#################################################
# 5. Self-Supervised Pre-training Module
#################################################

class MaskedCircuitModeling(nn.Module):
    """ë§ˆìŠ¤í¬ëœ íšŒë¡œ ëª¨ë¸ë§ì„ í†µí•œ ìê¸°ì§€ë„ í•™ìŠµ"""
    
    def __init__(self, d_model: int, vocab_size: int = 10):
        super().__init__()
        self.d_model = d_model
        self.vocab_size = vocab_size
        
        # ë§ˆìŠ¤í¬ í† í° ì„ë² ë”©
        self.mask_token = nn.Parameter(torch.randn(d_model))
        
        # ê²Œì´íŠ¸ ì˜ˆì¸¡ í—¤ë“œ
        self.gate_prediction_head = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.GELU(),
            nn.LayerNorm(d_model),
            nn.Linear(d_model, vocab_size)
        )
        
        # íŒŒë¼ë¯¸í„° ì˜ˆì¸¡ í—¤ë“œ
        self.param_prediction_head = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.GELU(),
            nn.Linear(d_model // 2, 1)
        )
    
    def create_masked_input(self, gate_sequence, param_sequence, mask_prob=0.15):
        batch_size, seq_len = gate_sequence.shape
        
        # ë§ˆìŠ¤í‚¹í•  ìœ„ì¹˜ ì„ íƒ
        mask = torch.rand(batch_size, seq_len) < mask_prob
        
        # ë§ˆìŠ¤í¬ëœ ê²Œì´íŠ¸ ì‹œí€€ìŠ¤
        masked_gates = gate_sequence.clone()
        masked_gates[mask] = 0  # 0ì€ ë§ˆìŠ¤í¬ í† í°
        
        # ë§ˆìŠ¤í¬ëœ íŒŒë¼ë¯¸í„° ì‹œí€€ìŠ¤
        masked_params = param_sequence.clone()
        masked_params[mask] = 0.0
        
        return masked_gates, masked_params, mask
    
    def forward(self, hidden_states, original_gates, original_params, mask):
        # ë§ˆìŠ¤í¬ëœ ìœ„ì¹˜ì˜ ê²Œì´íŠ¸ ì˜ˆì¸¡
        gate_predictions = self.gate_prediction_head(hidden_states)
        
        # ë§ˆìŠ¤í¬ëœ ìœ„ì¹˜ì˜ íŒŒë¼ë¯¸í„° ì˜ˆì¸¡
        param_predictions = self.param_prediction_head(hidden_states).squeeze(-1)
        
        # ì†ì‹¤ ê³„ì‚° (ë§ˆìŠ¤í¬ëœ ìœ„ì¹˜ì—ì„œë§Œ)
        gate_loss = F.cross_entropy(
            gate_predictions[mask], 
            original_gates[mask], 
            ignore_index=0
        )
        
        param_loss = F.mse_loss(
            param_predictions[mask], 
            original_params[mask]
        )
        
        return gate_loss + param_loss

#################################################
# 6. Uncertainty Quantification
#################################################

class BayesianLinear(nn.Module):
    """ë² ì´ì§€ì•ˆ ì„ í˜• ë ˆì´ì–´ (ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”)"""
    
    def __init__(self, in_features: int, out_features: int):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        
        # ê°€ì¤‘ì¹˜ í‰ê· ê³¼ ë¶„ì‚°
        self.weight_mu = nn.Parameter(torch.randn(out_features, in_features))
        self.weight_logvar = nn.Parameter(torch.randn(out_features, in_features))
        
        # í¸í–¥ í‰ê· ê³¼ ë¶„ì‚°
        self.bias_mu = nn.Parameter(torch.randn(out_features))
        self.bias_logvar = nn.Parameter(torch.randn(out_features))
        
        # ì‚¬ì „ ë¶„í¬ íŒŒë¼ë¯¸í„°
        self.prior_mu = 0.0
        self.prior_logvar = 0.0
    
    def forward(self, x, sample=True):
        if sample:
            # ê°€ì¤‘ì¹˜ ìƒ˜í”Œë§
            weight_std = torch.exp(0.5 * self.weight_logvar)
            weight_eps = torch.randn_like(weight_std)
            weight = self.weight_mu + weight_eps * weight_std
            
            # í¸í–¥ ìƒ˜í”Œë§
            bias_std = torch.exp(0.5 * self.bias_logvar)
            bias_eps = torch.randn_like(bias_std)
            bias = self.bias_mu + bias_eps * bias_std
        else:
            weight = self.weight_mu
            bias = self.bias_mu
        
        return F.linear(x, weight, bias)
    
    def kl_divergence(self):
        """KL ë°œì‚° ê³„ì‚°"""
        # prior_logvarë¥¼ í…ì„œë¡œ ë³€í™˜
        prior_logvar_tensor = torch.tensor(self.prior_logvar, device=self.weight_mu.device, dtype=self.weight_mu.dtype)
        prior_mu_tensor = torch.tensor(self.prior_mu, device=self.weight_mu.device, dtype=self.weight_mu.dtype)
        
        kl_weight = 0.5 * torch.sum(
            self.weight_logvar - prior_logvar_tensor +
            (torch.exp(prior_logvar_tensor) + (self.weight_mu - prior_mu_tensor)**2) /
            torch.exp(self.weight_logvar) - 1
        )
        
        kl_bias = 0.5 * torch.sum(
            self.bias_logvar - prior_logvar_tensor +
            (torch.exp(prior_logvar_tensor) + (self.bias_mu - prior_mu_tensor)**2) /
            torch.exp(self.bias_logvar) - 1
        )
        
        return kl_weight + kl_bias

#################################################
# 7. Advanced Main Model
#################################################

class QuantumRepresentationTransformer(nn.Module):
    """ì–‘ì íšŒë¡œì˜ í‘œí˜„ í•™ìŠµì— íŠ¹í™”ëœ íŠ¸ëœìŠ¤í¬ë¨¸ (ì˜ˆì¸¡ í—¤ë“œ ìµœì†Œí™”)"""
    
    def __init__(self, 
                 vocab_size: int = 10,
                 d_model: int = LATENT_DIM,
                 num_heads: int = NUM_HEADS,
                 num_layers: int = NUM_LAYERS,
                 max_qubits: int = 127,
                 feature_dim: int = 33):
        super().__init__()
        
        self.d_model = d_model
        self.num_layers = num_layers
        
        # ì„ë² ë”© ë ˆì´ì–´ë“¤
        self.gate_embedding = nn.Embedding(vocab_size, d_model)
        self.qubit_embedding = nn.Embedding(max_qubits + 1, d_model)
        self.param_projection = nn.Linear(1, d_model)
        
        # ê³ ê¸‰ ìœ„ì¹˜ ì¸ì½”ë”©
        self.pos_encoding = QuantumAwarePositionalEncoding(d_model)
        
        # ê·¸ë˜í”„ ì‹ ê²½ë§ (í† í´ë¡œì§€ ì¸ì½”ë”©)
        self.topology_encoder = QuantumTopologyEncoder(d_model, d_model)
        
        # ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë“¤
        self.transformer_layers = nn.ModuleList([
            nn.ModuleDict({
                'multi_scale_attention': MultiScaleAttention(d_model, num_heads),
                'feed_forward': nn.Sequential(
                    nn.Linear(d_model, d_model * 4),
                    nn.GELU(),
                    nn.Dropout(0.1),
                    nn.Linear(d_model * 4, d_model),
                    nn.Dropout(0.1)
                ),
                'norm1': nn.LayerNorm(d_model),
                'norm2': nn.LayerNorm(d_model),
                'physics_constraints': QuantumPhysicsConstraints(d_model)
            }) for _ in range(num_layers)
        ])
        
        # íŠ¹ì„± ìœµí•© ë„¤íŠ¸ì›Œí¬
        self.feature_encoder = nn.Sequential(
            nn.Linear(feature_dim, d_model),
            nn.GELU(),
            nn.LayerNorm(d_model),
            nn.Linear(d_model, d_model)
        )
        
        # í¬ë¡œìŠ¤ ì–´í…ì…˜ (ì‹œí€€ìŠ¤ â†” íŠ¹ì„±)
        self.cross_attention = nn.MultiheadAttention(
            d_model, num_heads, dropout=0.1, batch_first=True
        )
        
        # ìê¸°ì§€ë„ í•™ìŠµ ëª¨ë“ˆ
        self.masked_modeling = MaskedCircuitModeling(d_model, vocab_size)
        
        # ğŸ¯ ë‹¨ìˆœí™”ëœ í‘œí˜„ ì¶”ì¶œê¸° (ì˜ˆì¸¡ í—¤ë“œ ëŒ€ì‹ )
        self.representation_head = nn.Sequential(
            nn.Linear(d_model * 3 + feature_dim, d_model),
            nn.GELU(),
            nn.LayerNorm(d_model),
            nn.Dropout(0.1)
        )
        
        # ì„ íƒì  ì˜ˆì¸¡ ë ˆì´ì–´ (í•„ìš”ì‹œì—ë§Œ ì‚¬ìš©)
        self.optional_predictor = nn.Linear(d_model, 3)  # ë‹¨ì¼ ì˜ˆì¸¡ í—¤ë“œ
        
        # ì´ˆê¸°í™”
        self._init_weights()
    
    def get_circuit_representation(self, gate_sequence, qubit_sequence, param_sequence, 
                                  gate_type_sequence, features):
        """ì–‘ì íšŒë¡œì˜ ê³ ì°¨ì› í‘œí˜„ ë²¡í„° ì¶”ì¶œ (ì˜ˆì¸¡ ì—†ì´)"""
        batch_size, seq_len = gate_sequence.shape
        
        # ì„ë² ë”©
        gate_emb = self.gate_embedding(gate_sequence)
        qubit_emb = self.qubit_embedding(qubit_sequence.clamp(0, 127))
        param_emb = self.param_projection(param_sequence.unsqueeze(-1))
        
        # ì‹œí€€ìŠ¤ ì„ë² ë”© ê²°í•©
        sequence_emb = gate_emb + qubit_emb + param_emb
        
        # ê³ ê¸‰ ìœ„ì¹˜ ì¸ì½”ë”©
        depths = torch.arange(seq_len, device=gate_sequence.device).unsqueeze(0).expand(batch_size, -1)
        sequence_emb = self.pos_encoding(sequence_emb, gate_sequence, qubit_sequence, depths)
        
        # ê·¸ë˜í”„ í† í´ë¡œì§€ ì¸ì½”ë”©
        adjacency = self.create_adjacency_matrix(gate_sequence, qubit_sequence)
        node_features = torch.zeros(batch_size, 127, self.d_model, device=gate_sequence.device)
        
        # íë¹—ë³„ íŠ¹ì„± ì§‘ê³„
        for b in range(batch_size):
            for i in range(seq_len):
                qubit = qubit_sequence[b, i].item()
                if 0 <= qubit < 127:
                    node_features[b, qubit] += sequence_emb[b, i]
        
        topology_features = self.topology_encoder(node_features, adjacency)
        
        # íŠ¹ì„± ì¸ì½”ë”©
        feature_emb = self.feature_encoder(features).unsqueeze(1)
        
        # ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë“¤
        hidden_states = sequence_emb
        physics_scores = []
        
        for layer in self.transformer_layers:
            # ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì–´í…ì…˜
            attended = layer['multi_scale_attention'](hidden_states)
            hidden_states = layer['norm1'](attended + hidden_states)
            
            # í”¼ë“œí¬ì›Œë“œ
            ff_out = layer['feed_forward'](hidden_states)
            hidden_states = layer['norm2'](ff_out + hidden_states)
            
            # ë¬¼ë¦¬í•™ ì œì•½ ì ìš©
            hidden_states, physics_score, _ = layer['physics_constraints'](hidden_states)
            physics_scores.append(physics_score.mean())
        
        # ì‹œí€€ìŠ¤ì™€ íŠ¹ì„± ê°„ í¬ë¡œìŠ¤ ì–´í…ì…˜
        cross_attended, _ = self.cross_attention(
            hidden_states, 
            feature_emb.expand(-1, seq_len, -1), 
            feature_emb.expand(-1, seq_len, -1)
        )
        
        # ğŸ¯ ìµœì¢… í‘œí˜„ ë²¡í„° ìƒì„± (ì˜ˆì¸¡ ì—†ì´)
        seq_mean = hidden_states.mean(dim=1)  # [batch_size, d_model]
        seq_max = hidden_states.max(dim=1)[0]  # [batch_size, d_model]
        
        if topology_features.dim() == 3:
            topology_repr = topology_features.mean(dim=1)
        else:
            topology_repr = topology_features
        
        if features.dim() == 1:
            features = features.unsqueeze(0)
        
        # ëª¨ë“  í‘œí˜„ ê²°í•©
        combined_repr = torch.cat([
            seq_mean,
            seq_max, 
            topology_repr,
            features
        ], dim=-1)
        
        # ìµœì¢… í‘œí˜„ ë²¡í„°
        circuit_representation = self.representation_head(combined_repr)
        
        # ë¬¼ë¦¬í•™ ì ìˆ˜ í‰ê· 
        avg_physics_score = torch.stack(physics_scores).mean() if physics_scores else torch.tensor(0.0, device=gate_sequence.device)
        
        return circuit_representation, avg_physics_score
    
    def forward(self, gate_sequence, qubit_sequence, param_sequence, gate_type_sequence, features, 
                return_predictions=False, training_mode='representation'):
        """
        Args:
            return_predictions: Trueë©´ ì˜ˆì¸¡ê°’ë„ ë°˜í™˜, Falseë©´ í‘œí˜„ ë²¡í„°ë§Œ ë°˜í™˜
            training_mode: 'representation' (í‘œí˜„ í•™ìŠµ) ë˜ëŠ” 'self_supervised'
        """
        
        # í‘œí˜„ ë²¡í„° ì¶”ì¶œ
        circuit_repr, physics_score = self.get_circuit_representation(
            gate_sequence, qubit_sequence, param_sequence, gate_type_sequence, features
        )
        
        # ìê¸°ì§€ë„ í•™ìŠµ ì†ì‹¤ (í•„ìš”ì‹œ)
        ssl_loss = 0
        if training_mode == 'self_supervised':
            # ë§ˆìŠ¤í‚¹ ë° ì¬êµ¬ì„± ì†ì‹¤ ê³„ì‚°
            masked_gates, masked_params, mask = self.masked_modeling.create_masked_input(
                gate_sequence, param_sequence
            )
            # ... SSL ë¡œì§
        
        if return_predictions:
            # ì„ íƒì ìœ¼ë¡œ ì˜ˆì¸¡ê°’ ê³„ì‚°
            predictions = self.optional_predictor(circuit_repr)
            return circuit_repr, predictions, physics_score
        else:
            # í‘œí˜„ ë²¡í„°ë§Œ ë°˜í™˜
            return circuit_repr, physics_score
    
    def create_adjacency_matrix(self, gate_sequence, qubit_sequence):
        """íë¹— ê°„ ì—°ê²°ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì¸ì ‘ í–‰ë ¬ ìƒì„±"""
        batch_size, seq_len = gate_sequence.shape
        max_qubits = 127
        
        adjacency = torch.zeros(batch_size, max_qubits, max_qubits, device=gate_sequence.device)
        
        for b in range(batch_size):
            for i in range(seq_len):
                gate = gate_sequence[b, i].item()
                qubit = qubit_sequence[b, i].item()
                
                if gate == 8 and i + 1 < seq_len:  # CNOT ê²Œì´íŠ¸
                    qubit2 = qubit_sequence[b, i + 1].item()
                    if 0 <= qubit < max_qubits and 0 <= qubit2 < max_qubits:
                        adjacency[b, qubit, qubit2] = 1
                        adjacency[b, qubit2, qubit] = 1
        
        return adjacency
    
    def _init_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, mean=0, std=0.02)


# ğŸ¯ í‘œí˜„ë ¥ ê³„ì‚°ê³¼ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ê²°í•©í•˜ëŠ” í†µí•© ì‹œìŠ¤í…œ
class QuantumExpressibilitySystem:
    """íŠ¸ëœìŠ¤í¬ë¨¸ í‘œí˜„ + Classical Shadow + ì—”íŠ¸ë¡œí”¼ ë°©ë²•ì„ í†µí•©"""
    
    def __init__(self, transformer_model):
        self.transformer = transformer_model
        
    def compute_comprehensive_expressibility(self, gate_sequence, qubit_sequence, 
                                           param_sequence, gate_type_sequence, features,
                                           measurement_counts=None):
        """
        ì¢…í•©ì ì¸ í‘œí˜„ë ¥ ê³„ì‚°:
        1. íŠ¸ëœìŠ¤í¬ë¨¸ í‘œí˜„ ë²¡í„°
        2. Classical Shadow í‘œí˜„ë ¥
        3. ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ í‘œí˜„ë ¥
        """
        
        # 1. íŠ¸ëœìŠ¤í¬ë¨¸ í‘œí˜„ ë²¡í„° ì¶”ì¶œ
        circuit_repr, physics_score = self.transformer.get_circuit_representation(
            gate_sequence, qubit_sequence, param_sequence, gate_type_sequence, features
        )
        
        # 2. Classical Shadow í‘œí˜„ë ¥ (ê¸°ì¡´ ë°©ë²•)
        classical_shadow_expr = 0  # ì‹¤ì œ ê³„ì‚° ë¡œì§ í•„ìš”
        
        # 3. ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ í‘œí˜„ë ¥ (ìƒˆë¡œìš´ ë°©ë²•)
        entropy_expr = 0  # ì‹¤ì œ ê³„ì‚° ë¡œì§ í•„ìš”
        
        # 4. íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ í‘œí˜„ë ¥ (í‘œí˜„ ë²¡í„°ì˜ ë‹¤ì–‘ì„±)
        transformer_expr = self._compute_transformer_expressibility(circuit_repr)
        
        return {
            'transformer_representation': circuit_repr,
            'classical_shadow_expressibility': classical_shadow_expr,
            'entropy_expressibility': entropy_expr,
            'transformer_expressibility': transformer_expr,
            'physics_score': physics_score
        }
    
    def _compute_transformer_expressibility(self, circuit_repr):
        """íŠ¸ëœìŠ¤í¬ë¨¸ í‘œí˜„ ë²¡í„°ì˜ ë‹¤ì–‘ì„±ì„ ì¸¡ì •"""
        # í‘œí˜„ ë²¡í„°ì˜ ì—”íŠ¸ë¡œí”¼ë‚˜ ë¶„ì‚°ì„ ê³„ì‚°
        return torch.var(circuit_repr, dim=-1).mean().item()

#################################################
# 8. Advanced Training Loop
#################################################

def train_advanced_model():
    """ê³ ê¸‰ ëª¨ë¸ í›ˆë ¨"""
    print("ğŸš€ Advanced Quantum Transformer í›ˆë ¨ ì‹œì‘!")
    
    # ë°ì´í„° ë¡œë“œ
    data_dir = "grid_ansatz/grid_circuits/training_data"
    if not os.path.exists(data_dir):
        print(f"âŒ í›ˆë ¨ ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        return
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    model = QuantumRepresentationTransformer().to(DEVICE)
    
    # ì˜µí‹°ë§ˆì´ì € (AdamW + ì½”ì‚¬ì¸ ì–´ë‹ë§)
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)
    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)
    
    # ì†ì‹¤ í•¨ìˆ˜ë“¤
    mse_loss = nn.MSELoss()
    
    print("ğŸ¯ í›ˆë ¨ ì‹œì‘!")
    
    # 3ë‹¨ê³„ í›ˆë ¨ ì „ëµ
    for phase in ['self_supervised', 'supervised', 'fine_tuning']:
        print(f"\nğŸ“š {phase.upper()} ë‹¨ê³„ ì‹œì‘")
        
        if phase == 'self_supervised':
            epochs = 50
            training_mode = 'self_supervised'
        elif phase == 'supervised':
            epochs = 100
            training_mode = 'supervised'
        else:  # fine_tuning
            epochs = 50
            training_mode = 'supervised'
            # í•™ìŠµë¥  ê°ì†Œ
            for param_group in optimizer.param_groups:
                param_group['lr'] *= 0.1
        
        for epoch in range(epochs):
            model.train()
            total_loss = 0
            physics_scores = []
            
            # ì—¬ê¸°ì„œ ì‹¤ì œ ë°ì´í„° ë¡œë”ë¥¼ êµ¬í˜„í•´ì•¼ í•¨
            # í˜„ì¬ëŠ” ë”ë¯¸ ë°ì´í„°ë¡œ ì‹œì—°
            for batch_idx in range(10):  # ë”ë¯¸ ë°°ì¹˜
                # ë”ë¯¸ ë°ì´í„° ìƒì„±
                batch_size = BATCH_SIZE
                gate_seq = torch.randint(1, 9, (batch_size, MAX_SEQUENCE_LENGTH)).to(DEVICE)
                qubit_seq = torch.randint(0, 20, (batch_size, MAX_SEQUENCE_LENGTH)).to(DEVICE)
                param_seq = torch.randn(batch_size, MAX_SEQUENCE_LENGTH).to(DEVICE)
                gate_type_seq = torch.randint(0, 3, (batch_size, MAX_SEQUENCE_LENGTH)).to(DEVICE)
                features = torch.randn(batch_size, 33).to(DEVICE)
                targets = torch.randn(batch_size, 3).to(DEVICE)
                
                optimizer.zero_grad()
                
                if training_mode == 'self_supervised':
                    predictions, ssl_loss, physics_score = model(
                        gate_seq, qubit_seq, param_seq, gate_type_seq, features,
                        training_mode='self_supervised'
                    )
                    loss = ssl_loss
                else:
                    predictions, physics_score = model(
                        gate_seq, qubit_seq, param_seq, gate_type_seq, features,
                        training_mode='supervised'
                    )
                    loss = mse_loss(predictions, targets)
                    
                    # ë¬¼ë¦¬í•™ ì œì•½ ì†ì‹¤ ì¶”ê°€
                    physics_loss = (1.0 - physics_score) * 0.1
                    loss += physics_loss
                
                # ë² ì´ì§€ì•ˆ ë ˆì´ì–´ì˜ KL ë°œì‚° ì¶”ê°€
                kl_loss = 0
                for module in model.modules():
                    if isinstance(module, BayesianLinear):
                        kl_loss += module.kl_divergence()
                
                loss += kl_loss * 0.001  # KL ê°€ì¤‘ì¹˜
                
                loss.backward()
                
                # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
                
                total_loss += loss.item()
                physics_scores.append(physics_score.item())
            
            scheduler.step()
            
            avg_loss = total_loss / 10
            avg_physics = np.mean(physics_scores)
            
            if epoch % 10 == 0:
                print(f"  Epoch {epoch}: Loss = {avg_loss:.6f}, Physics Score = {avg_physics:.4f}")
    
    # ëª¨ë¸ ì €ì¥
    torch.save(model.state_dict(), "advanced_quantum_transformer.pth")
    print("\nğŸ’¾ ê³ ê¸‰ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: advanced_quantum_transformer.pth")
    
    return model

if __name__ == "__main__":
    print("ğŸš€ Advanced Quantum AI ì‹œìŠ¤í…œ ì‹œì‘!")
    print("\nì„ íƒí•  ìˆ˜ ìˆëŠ” ëª¨ë¸:")
    print("1. í‘œí˜„ í•™ìŠµ íŠ¸ëœìŠ¤í¬ë¨¸ (QuantumRepresentationTransformer)")
    print("2. ë””í“¨ì „ íŠ¸ëœìŠ¤í¬ë¨¸ (QuantumCircuitDiT)")
    print("3. í†µí•© ì‹œìŠ¤í…œ (ëª¨ë“  ëª¨ë¸)")
    
    choice = input("\nëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš” (1/2/3): ").strip()
    
    if choice == "1":
        # í‘œí˜„ í•™ìŠµ íŠ¸ëœìŠ¤í¬ë¨¸ í›ˆë ¨
        print("\nğŸ¯ í‘œí˜„ í•™ìŠµ íŠ¸ëœìŠ¤í¬ë¨¸ í›ˆë ¨ ì‹œì‘!")
        model = train_advanced_model()
        
    elif choice == "2":
        # ë””í“¨ì „ íŠ¸ëœìŠ¤í¬ë¨¸ í›ˆë ¨
        print("\nğŸ¯ ë””í“¨ì „ íŠ¸ëœìŠ¤í¬ë¨¸ í›ˆë ¨ ì‹œì‘!")
        model, diffusion = train_quantum_diffusion_model()
        
        # ìƒ˜í”Œ ìƒì„± ë°ëª¨
        print("\nğŸ¨ ì–‘ì íšŒë¡œ ìƒì„± ë°ëª¨:")
        with torch.no_grad():
            model.eval()
            # ë‹¤ì–‘í•œ ì¡°ê±´ìœ¼ë¡œ íšŒë¡œ ìƒì„±
            conditions = torch.tensor([5, 15, 25, 35], device=DEVICE)  # ë‹¤ë¥¸ ë³µì¡ë„
            generated_circuits = diffusion.sample((4, 30), conditions, num_inference_steps=50)
            
            for i, condition in enumerate(conditions):
                print(f"\nì¡°ê±´ {condition.item()}ìœ¼ë¡œ ìƒì„±ëœ íšŒë¡œ:")
                print(f"  ê²Œì´íŠ¸ ì‹œí€€ìŠ¤: {generated_circuits['gate_sequence'][i][:15].tolist()}")
                print(f"  íë¹— ì‹œí€€ìŠ¤: {generated_circuits['qubit_sequence'][i][:15].tolist()}")
                print(f"  íŒŒë¼ë¯¸í„°: {generated_circuits['param_sequence'][i][:5].tolist()}")
        
    elif choice == "3":
        # í†µí•© ì‹œìŠ¤í…œ
        print("\nğŸ¯ í†µí•© AI ì‹œìŠ¤í…œ í›ˆë ¨ ì‹œì‘!")
        
        # 1. í‘œí˜„ í•™ìŠµ íŠ¸ëœìŠ¤í¬ë¨¸
        print("\n1ï¸âƒ£ í‘œí˜„ í•™ìŠµ íŠ¸ëœìŠ¤í¬ë¨¸ í›ˆë ¨...")
        repr_model = train_advanced_model()
        
        # 2. ë””í“¨ì „ íŠ¸ëœìŠ¤í¬ë¨¸  
        print("\n2ï¸âƒ£ ë””í“¨ì „ íŠ¸ëœìŠ¤í¬ë¨¸ í›ˆë ¨...")
        dit_model, diffusion = train_quantum_diffusion_model()
        
        # 3. í†µí•© í‘œí˜„ë ¥ ì‹œìŠ¤í…œ
        print("\n3ï¸âƒ£ í†µí•© í‘œí˜„ë ¥ ì‹œìŠ¤í…œ êµ¬ì¶•...")
        expressibility_system = QuantumExpressibilitySystem(repr_model)
        
        print("\nğŸ‰ í†µí•© ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ!")
        print("\nğŸ’¡ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥:")
        print("  âœ… ì–‘ì íšŒë¡œ í‘œí˜„ í•™ìŠµ (QuantumRepresentationTransformer)")
        print("  âœ… ì–‘ì íšŒë¡œ ìƒì„± (QuantumCircuitDiT)")
        print("  âœ… ì¢…í•© í‘œí˜„ë ¥ ë¶„ì„ (QuantumExpressibilitySystem)")
        print("  âœ… Classical Shadow í‘œí˜„ë ¥")
        print("  âœ… ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ í‘œí˜„ë ¥")
        print("  âœ… íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ í‘œí˜„ë ¥")
        
        # í†µí•© ë°ëª¨
        print("\nğŸ¨ í†µí•© ì‹œìŠ¤í…œ ë°ëª¨:")
        with torch.no_grad():
            # íšŒë¡œ ìƒì„±
            dit_model.eval()
            conditions = torch.tensor([10, 20], device=DEVICE)
            generated = diffusion.sample((2, 25), conditions, num_inference_steps=30)
            
            # ìƒì„±ëœ íšŒë¡œì˜ í‘œí˜„ ë¶„ì„
            repr_model.eval()
            dummy_features = torch.randn(2, 33, device=DEVICE)
            circuit_repr, physics_score = repr_model.get_circuit_representation(
                generated['gate_sequence'],
                generated['qubit_sequence'], 
                generated['param_sequence'],
                generated['gate_sequence'],  # gate_type_sequence
                dummy_features
            )
            
            print(f"  ìƒì„±ëœ íšŒë¡œ ìˆ˜: {generated['gate_sequence'].shape[0]}")
            print(f"  í‘œí˜„ ë²¡í„° ì°¨ì›: {circuit_repr.shape}")
            print(f"  ë¬¼ë¦¬í•™ ì¼ê´€ì„± ì ìˆ˜: {physics_score.item():.4f}")
    
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 1, 2, ë˜ëŠ” 3ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    print("\nğŸ‰ Advanced Quantum AI ì‹œìŠ¤í…œ ì™„ë£Œ!")
    print("\nğŸ”¬ êµ¬í˜„ëœ ìµœì²¨ë‹¨ AI ê¸°ë²•ë“¤:")
    print("  âœ… Multi-Head Cross-Attention")
    print("  âœ… Graph Neural Networks for Quantum Topology")
    print("  âœ… Hierarchical Multi-Scale Attention")
    print("  âœ… Physics-Informed Neural Networks")
    print("  âœ… Self-Supervised Pre-training")
    print("  âœ… Bayesian Uncertainty Quantification")
    print("  âœ… Monte Carlo Dropout")
    print("  âœ… Advanced Positional Encoding")
    print("  âœ… Knowledge Distillation from Physics Priors")
    print("  âœ… Multi-Modal Fusion")
    print("  âœ… Diffusion Transformer (DiT)")
    print("  âœ… Adaptive Layer Normalization (adaLN)")
    print("  âœ… Classifier-Free Guidance")
    print("  âœ… Cosine Beta Scheduling") 