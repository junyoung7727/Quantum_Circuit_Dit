#!/usr/bin/env python3
"""
ì–‘ì íšŒë¡œ í‘œí˜„ë ¥ ë° í”¼ë¸ë¦¬í‹° ì˜ˆì¸¡ì„ ìœ„í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸
ìš°ë¦¬ì˜ mega job ë°ì´í„° êµ¬ì¡°ì— ìµœì í™”ë¨
"""

import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import json
import gzip
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd

# ì„¤ì • íŒŒë¼ë¯¸í„°
MAX_SEQUENCE_LENGTH = 100  # ìš°ë¦¬ ë°ì´í„°ì˜ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
LATENT_DIM = 256
NUM_HEADS = 8
NUM_LAYERS = 6
BATCH_SIZE = 32
NUM_EPOCHS = 100
LEARNING_RATE = 1e-4
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#################################################
# 1. ìš°ë¦¬ ë°ì´í„° êµ¬ì¡°ì— ë§ëŠ” ë°ì´í„°ì…‹
#################################################

class QuantumMegaJobDataset(Dataset):
    def __init__(self, data_dir, split='train', train_ratio=0.8):
        """
        Mega Job ë°ì´í„°ì…‹ ë¡œë“œ
        data_dir: training_data ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        self.data_dir = data_dir
        self.split = split
        
        # ë°°ì¹˜ íŒŒì¼ë“¤ ë¡œë“œ
        self.batch_files = [f for f in os.listdir(data_dir) 
                           if f.startswith('mega_batch_') and f.endswith('.json.gz')]
        
        # í›ˆë ¨/ê²€ì¦ ë¶„í• 
        np.random.shuffle(self.batch_files)
        split_idx = int(len(self.batch_files) * train_ratio)
        
        if split == 'train':
            self.batch_files = self.batch_files[:split_idx]
        else:
            self.batch_files = self.batch_files[split_idx:]
        
        # ëª¨ë“  íšŒë¡œ ë°ì´í„° ë¡œë“œ
        self.circuits = []
        self._load_all_circuits()
        
        print(f"{split} ë°ì´í„°ì…‹: {len(self.circuits)}ê°œ íšŒë¡œ ë¡œë“œë¨")
    
    def _load_all_circuits(self):
        """ëª¨ë“  ë°°ì¹˜ íŒŒì¼ì—ì„œ íšŒë¡œ ë°ì´í„° ë¡œë“œ"""
        for batch_file in tqdm(self.batch_files, desc=f"{self.split} ë°ì´í„° ë¡œë“œ ì¤‘"):
            batch_path = os.path.join(self.data_dir, batch_file)
            
            try:
                with gzip.open(batch_path, 'rt') as f:
                    batch_data = json.load(f)
                
                # ê° íšŒë¡œ ë°ì´í„° ì¶”ì¶œ
                for circuit in batch_data['circuits']:
                    self.circuits.append(circuit)
                    
            except Exception as e:
                print(f"ë°°ì¹˜ íŒŒì¼ {batch_file} ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    
    def __len__(self):
        return len(self.circuits)
    
    def __getitem__(self, idx):
        circuit = self.circuits[idx]
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ì…ë ¥ ë°ì´í„° ì¶”ì¶œ
        transformer_input = circuit.get('transformer_input', {})
        
        # ì‹œí€€ìŠ¤ ë°ì´í„° (íŒ¨ë”© ì²˜ë¦¬ë¨)
        gate_sequence = torch.LongTensor(transformer_input.get('gate_sequence', [0] * MAX_SEQUENCE_LENGTH))
        qubit_sequence = torch.LongTensor(transformer_input.get('qubit_sequence', [-1] * MAX_SEQUENCE_LENGTH))
        param_sequence = torch.FloatTensor(transformer_input.get('param_sequence', [0.0] * MAX_SEQUENCE_LENGTH))
        gate_type_sequence = torch.LongTensor(transformer_input.get('gate_type_sequence', [0] * MAX_SEQUENCE_LENGTH))
        
        # íŠ¹ì„± ë°ì´í„°
        features = circuit.get('features', {})
        
        # êµ¬ì¡°ì  íŠ¹ì„±
        structural_features = torch.FloatTensor([
            features.get('n_qubits', 0) / 127.0,  # ì •ê·œí™” (ìµœëŒ€ 127íë¹—)
            features.get('depth', 0) / 10.0,      # ì •ê·œí™” (ìµœëŒ€ ê¹Šì´ 10)
            features.get('gate_count', 0) / 200.0, # ì •ê·œí™” (ìµœëŒ€ ê²Œì´íŠ¸ 200ê°œ)
            features.get('cnot_count', 0) / 100.0, # ì •ê·œí™”
            features.get('single_qubit_gates', 0) / 150.0, # ì •ê·œí™”
            features.get('unique_gate_types', 0) / 8.0,    # ì •ê·œí™”
            features.get('cnot_connections', 0) / 50.0,    # ì •ê·œí™”
        ])
        
        # ì»¤í”Œë§ íŠ¹ì„±
        coupling_features = torch.FloatTensor([
            features.get('coupling_density', 0.0),
            features.get('max_degree', 0) / 10.0,  # ì •ê·œí™”
            features.get('avg_degree', 0.0) / 5.0, # ì •ê·œí™”
            features.get('connectivity_ratio', 0.0),
            features.get('diameter', 0) / 20.0,    # ì •ê·œí™”
            features.get('clustering_coefficient', 0.0),
        ])
        
        # íŒŒë¼ë¯¸í„° íŠ¹ì„±
        param_features = torch.FloatTensor([
            features.get('param_count', 0) / 50.0,  # ì •ê·œí™”
            features.get('param_mean', 0.0) / (2 * np.pi),  # ì •ê·œí™”
            features.get('param_std', 0.0) / np.pi,         # ì •ê·œí™”
            features.get('param_min', 0.0) / (2 * np.pi),   # ì •ê·œí™”
            features.get('param_max', 0.0) / (2 * np.pi),   # ì •ê·œí™”
        ])
        
        # ì¸¡ì • í†µê³„ íŠ¹ì„±
        measurement_features = torch.FloatTensor([
            features.get('entropy', 0.0) / 20.0,  # ì •ê·œí™” (ìµœëŒ€ ì—”íŠ¸ë¡œí”¼ ~20)
            features.get('zero_state_probability', 0.0),
            features.get('concentration', 0.0),
            features.get('measured_states', 0) / 1000.0,  # ì •ê·œí™”
            features.get('top_1_probability', 0.0),
            features.get('top_5_probability', 0.0),
            features.get('top_10_probability', 0.0),
        ])
        
        # ì‹œí€€ìŠ¤ íŠ¹ì„±
        sequence_features = torch.FloatTensor([
            features.get('sequence_length', 0) / MAX_SEQUENCE_LENGTH,
            features.get('unique_gates_in_sequence', 0) / 8.0,
            features.get('param_gate_ratio', 0.0),
            features.get('two_qubit_gate_ratio', 0.0),
        ])
        
        # í•˜ë“œì›¨ì–´ íŠ¹ì„±
        hardware_features = torch.FloatTensor([
            features.get('gate_overhead', 1.0),
            features.get('depth_overhead', 1.0),
            features.get('transpiled_depth', 0) / 50.0,  # ì •ê·œí™”
            features.get('transpiled_gate_count', 0) / 500.0,  # ì •ê·œí™”
        ])
        
        # ëª¨ë“  íŠ¹ì„± ê²°í•©
        combined_features = torch.cat([
            structural_features,
            coupling_features, 
            param_features,
            measurement_features,
            sequence_features,
            hardware_features
        ])
        
        # íƒ€ê²Ÿ ê°’ë“¤ (ì˜ˆì¸¡í•  ê°’ë“¤)
        targets = torch.FloatTensor([
            features.get('fidelity', 0.0),
            features.get('normalized_expressibility', 0.0),
            features.get('expressibility_distance', 0.0) / 1e-3,  # ì •ê·œí™”
        ])
        
        return {
            'gate_sequence': gate_sequence,
            'qubit_sequence': qubit_sequence, 
            'param_sequence': param_sequence,
            'gate_type_sequence': gate_type_sequence,
            'features': combined_features,
            'targets': targets,
            'circuit_id': circuit.get('circuit_id', 'unknown')
        }

#################################################
# 2. ìš°ë¦¬ ë°ì´í„°ì— ë§ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸
#################################################

class QuantumCircuitTransformer(nn.Module):
    def __init__(
        self,
        vocab_size=9,  # ê²Œì´íŠ¸ íƒ€ì… ìˆ˜ (0~8)
        max_qubits=127,
        sequence_length=MAX_SEQUENCE_LENGTH,
        d_model=LATENT_DIM,
        nhead=NUM_HEADS,
        num_layers=NUM_LAYERS,
        feature_dim=33,  # ê²°í•©ëœ íŠ¹ì„± ì°¨ì›
        output_dim=3,    # í”¼ë¸ë¦¬í‹°, ì •ê·œí™”ëœ í‘œí˜„ë ¥, í‘œí˜„ë ¥ ê±°ë¦¬
        dropout=0.1
    ):
        super().__init__()
        
        self.d_model = d_model
        self.sequence_length = sequence_length
        
        # ì„ë² ë”© ë ˆì´ì–´ë“¤
        self.gate_embedding = nn.Embedding(vocab_size, d_model // 4)
        self.qubit_embedding = nn.Embedding(max_qubits + 2, d_model // 4)  # +2 for padding (-1) and special tokens
        self.gate_type_embedding = nn.Embedding(3, d_model // 4)  # 0, 1, 2
        self.param_projection = nn.Linear(1, d_model // 4)
        
        # ì‹œí€€ìŠ¤ ê²°í•© í”„ë¡œì ì…˜
        self.sequence_projection = nn.Linear(d_model, d_model)
        
        # í¬ì§€ì…”ë„ ì¸ì½”ë”©
        self.pos_encoding = nn.Parameter(torch.randn(1, sequence_length, d_model))
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # íŠ¹ì„± ì²˜ë¦¬ ë„¤íŠ¸ì›Œí¬
        self.feature_network = nn.Sequential(
            nn.Linear(feature_dim, d_model),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model, d_model),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # ìµœì¢… ì˜ˆì¸¡ ë„¤íŠ¸ì›Œí¬
        self.prediction_head = nn.Sequential(
            nn.Linear(d_model * 2, d_model),  # ì‹œí€€ìŠ¤ + íŠ¹ì„±
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, output_dim)
        )
        
        # ë“œë¡­ì•„ì›ƒ
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, gate_seq, qubit_seq, param_seq, gate_type_seq, features):
        batch_size = gate_seq.size(0)
        
        # íë¹— ì‹œí€€ìŠ¤ ì²˜ë¦¬ (ìŒìˆ˜ ê°’ì„ íŠ¹ë³„í•œ í† í°ìœ¼ë¡œ ë³€í™˜)
        qubit_seq_processed = qubit_seq.clone()
        qubit_seq_processed[qubit_seq_processed < 0] = 127 + 1  # íŒ¨ë”© í† í°
        
        # ê° ì»´í¬ë„ŒíŠ¸ ì„ë² ë”©
        gate_emb = self.gate_embedding(gate_seq)  # [B, L, d_model//4]
        qubit_emb = self.qubit_embedding(qubit_seq_processed)  # [B, L, d_model//4]
        gate_type_emb = self.gate_type_embedding(gate_type_seq)  # [B, L, d_model//4]
        param_emb = self.param_projection(param_seq.unsqueeze(-1))  # [B, L, d_model//4]
        
        # ì‹œí€€ìŠ¤ ì„ë² ë”© ê²°í•©
        sequence_emb = torch.cat([gate_emb, qubit_emb, gate_type_emb, param_emb], dim=-1)  # [B, L, d_model]
        sequence_emb = self.sequence_projection(sequence_emb)
        
        # í¬ì§€ì…”ë„ ì¸ì½”ë”© ì¶”ê°€
        sequence_emb = sequence_emb + self.pos_encoding
        sequence_emb = self.dropout(sequence_emb)
        
        # íŒ¨ë”© ë§ˆìŠ¤í¬ ìƒì„± (ê²Œì´íŠ¸ ì‹œí€€ìŠ¤ê°€ 0ì¸ ìœ„ì¹˜)
        padding_mask = (gate_seq == 0)
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” í†µê³¼
        transformer_output = self.transformer(sequence_emb, src_key_padding_mask=padding_mask)
        
        # ì‹œí€€ìŠ¤ í‘œí˜„ (í‰ê·  í’€ë§, íŒ¨ë”© ì œì™¸)
        mask = (~padding_mask).float().unsqueeze(-1)
        sequence_repr = (transformer_output * mask).sum(dim=1) / mask.sum(dim=1)
        
        # íŠ¹ì„± ì²˜ë¦¬
        feature_repr = self.feature_network(features)
        
        # ì‹œí€€ìŠ¤ì™€ íŠ¹ì„± ê²°í•©
        combined_repr = torch.cat([sequence_repr, feature_repr], dim=-1)
        
        # ìµœì¢… ì˜ˆì¸¡
        predictions = self.prediction_head(combined_repr)
        
        return predictions

#################################################
# 3. í›ˆë ¨ ë° í‰ê°€ í•¨ìˆ˜ë“¤
#################################################

def train_model(model, train_loader, val_loader, num_epochs=NUM_EPOCHS):
    """ëª¨ë¸ í›ˆë ¨"""
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
    criterion = nn.MSELoss()
    
    train_losses = []
    val_losses = []
    
    best_val_loss = float('inf')
    
    for epoch in range(num_epochs):
        # í›ˆë ¨
        model.train()
        train_loss = 0.0
        train_batches = 0
        
        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Training"):
            optimizer.zero_grad()
            
            # ë°ì´í„° GPUë¡œ ì´ë™
            gate_seq = batch['gate_sequence'].to(DEVICE)
            qubit_seq = batch['qubit_sequence'].to(DEVICE)
            param_seq = batch['param_sequence'].to(DEVICE)
            gate_type_seq = batch['gate_type_sequence'].to(DEVICE)
            features = batch['features'].to(DEVICE)
            targets = batch['targets'].to(DEVICE)
            
            # ì˜ˆì¸¡
            predictions = model(gate_seq, qubit_seq, param_seq, gate_type_seq, features)
            
            # ì†ì‹¤ ê³„ì‚°
            loss = criterion(predictions, targets)
            
            # ì—­ì „íŒŒ
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_loss += loss.item()
            train_batches += 1
        
        # ê²€ì¦
        model.eval()
        val_loss = 0.0
        val_batches = 0
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Validation"):
                gate_seq = batch['gate_sequence'].to(DEVICE)
                qubit_seq = batch['qubit_sequence'].to(DEVICE)
                param_seq = batch['param_sequence'].to(DEVICE)
                gate_type_seq = batch['gate_type_sequence'].to(DEVICE)
                features = batch['features'].to(DEVICE)
                targets = batch['targets'].to(DEVICE)
                
                predictions = model(gate_seq, qubit_seq, param_seq, gate_type_seq, features)
                loss = criterion(predictions, targets)
                
                val_loss += loss.item()
                val_batches += 1
        
        # í‰ê·  ì†ì‹¤ ê³„ì‚°
        avg_train_loss = train_loss / train_batches
        avg_val_loss = val_loss / val_batches
        
        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
        scheduler.step()
        
        print(f"Epoch {epoch+1}/{num_epochs}")
        print(f"  Train Loss: {avg_train_loss:.6f}")
        print(f"  Val Loss: {avg_val_loss:.6f}")
        print(f"  LR: {scheduler.get_last_lr()[0]:.2e}")
        
        # ìµœê³  ëª¨ë¸ ì €ì¥
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), 'best_quantum_transformer.pth')
            print(f"  âœ… ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥! (Val Loss: {best_val_loss:.6f})")
        
        print()
    
    return train_losses, val_losses

def evaluate_model(model, test_loader):
    """ëª¨ë¸ í‰ê°€"""
    model.eval()
    
    all_predictions = []
    all_targets = []
    all_circuit_ids = []
    
    with torch.no_grad():
        for batch in tqdm(test_loader, desc="í‰ê°€ ì¤‘"):
            gate_seq = batch['gate_sequence'].to(DEVICE)
            qubit_seq = batch['qubit_sequence'].to(DEVICE)
            param_seq = batch['param_sequence'].to(DEVICE)
            gate_type_seq = batch['gate_type_sequence'].to(DEVICE)
            features = batch['features'].to(DEVICE)
            targets = batch['targets'].to(DEVICE)
            
            predictions = model(gate_seq, qubit_seq, param_seq, gate_type_seq, features)
            
            all_predictions.append(predictions.cpu().numpy())
            all_targets.append(targets.cpu().numpy())
            all_circuit_ids.extend(batch['circuit_id'])
    
    # ê²°ê³¼ ê²°í•©
    predictions = np.vstack(all_predictions)
    targets = np.vstack(all_targets)
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    target_names = ['Fidelity', 'Normalized Expressibility', 'Expressibility Distance']
    
    print("\nğŸ“Š í‰ê°€ ê²°ê³¼:")
    print("=" * 60)
    
    for i, name in enumerate(target_names):
        mse = mean_squared_error(targets[:, i], predictions[:, i])
        r2 = r2_score(targets[:, i], predictions[:, i])
        
        print(f"{name}:")
        print(f"  MSE: {mse:.6f}")
        print(f"  RÂ²: {r2:.4f}")
        print()
    
    return predictions, targets, all_circuit_ids

def plot_training_curves(train_losses, val_losses):
    """í›ˆë ¨ ê³¡ì„  ì‹œê°í™”"""
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Training Loss', alpha=0.8)
    plt.plot(val_losses, label='Validation Loss', alpha=0.8)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')
    plt.show()

def plot_predictions(predictions, targets, target_names):
    """ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”"""
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    for i, (name, ax) in enumerate(zip(target_names, axes)):
        ax.scatter(targets[:, i], predictions[:, i], alpha=0.6)
        ax.plot([targets[:, i].min(), targets[:, i].max()], 
                [targets[:, i].min(), targets[:, i].max()], 'r--', lw=2)
        ax.set_xlabel(f'True {name}')
        ax.set_ylabel(f'Predicted {name}')
        ax.set_title(f'{name} Prediction')
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('prediction_results.png', dpi=300, bbox_inches='tight')
    plt.show()

#################################################
# 4. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
#################################################

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ì–‘ì íšŒë¡œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ í›ˆë ¨ ì‹œì‘!")
    print(f"ë””ë°”ì´ìŠ¤: {DEVICE}")
    
    # ë°ì´í„° ë¡œë“œ
    data_dir = "grid_ansatz/grid_circuits/training_data"
    
    if not os.path.exists(data_dir):
        print(f"âŒ ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        print("ë¨¼ì € mega jobì„ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
        return
    
    # ë°ì´í„°ì…‹ ìƒì„±
    print("\nğŸ“‚ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    train_dataset = QuantumMegaJobDataset(data_dir, split='train')
    val_dataset = QuantumMegaJobDataset(data_dir, split='val')
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)
    
    # ëª¨ë¸ ìƒì„±
    print("\nğŸ¤– ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    model = QuantumCircuitTransformer().to(DEVICE)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")
    print(f"í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜: {trainable_params:,}")
    
    # ëª¨ë¸ í›ˆë ¨
    print("\nğŸ‹ï¸ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
    train_losses, val_losses = train_model(model, train_loader, val_loader)
    
    # í›ˆë ¨ ê³¡ì„  ì‹œê°í™”
    plot_training_curves(train_losses, val_losses)
    
    # ìµœê³  ëª¨ë¸ ë¡œë“œ
    print("\nğŸ“¥ ìµœê³  ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model.load_state_dict(torch.load('best_quantum_transformer.pth'))
    
    # í‰ê°€
    print("\nğŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘...")
    predictions, targets, circuit_ids = evaluate_model(model, val_loader)
    
    # ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
    target_names = ['Fidelity', 'Normalized Expressibility', 'Expressibility Distance']
    plot_predictions(predictions, targets, target_names)
    
    # ê²°ê³¼ ì €ì¥
    results_df = pd.DataFrame({
        'circuit_id': circuit_ids,
        'true_fidelity': targets[:, 0],
        'pred_fidelity': predictions[:, 0],
        'true_norm_expr': targets[:, 1],
        'pred_norm_expr': predictions[:, 1],
        'true_expr_dist': targets[:, 2],
        'pred_expr_dist': predictions[:, 2]
    })
    
    results_df.to_csv('prediction_results.csv', index=False)
    print("\nğŸ’¾ ê²°ê³¼ê°€ 'prediction_results.csv'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print("\nğŸ‰ í›ˆë ¨ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 